# 牛客网 - 算法/机器学习校招面试题目合集

本材料整理自[牛客网](https://www.nowcoder.com/ta/review-ml/)


## SGD,Momentum,Adagard,Adam原理

- SGD为随机梯度下降,每一次迭代计算数据集的mini-batch的梯度,然后对参数进行跟新。
- Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。
- Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。
- Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。


## L1不可导的时候该怎么办

-  当损失函数不可导,梯度下降不再有效,可以使用**坐标轴下降法**,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。
- 使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果。

##  sigmoid函数特性

- 定义域为 (-无穷, +无穷)
- 值域为(-1,1)
- 函数在定义域内为连续和光滑的函数
- 处处可导,导数为$f'(x)=f(x)(1-f(x))$

## 最大似然估计和最大后验概率的区别?

- 最大似然估计提供了一种给定观察数据来评估模型参数的方法,最大似然估计中的采样满足所有采样都是*独立同分布*的假设。
- 最大后验概率是根据经验数据获难以观察量的点估计,与最大似然估计最大的不同是最大后验概率融入了要估计量的先验分布在其中,所以最大后验概率可以看做规则化的最大似然估计。

## 什么是共轭先验分布

- 假设 \theta 为总体分布中的参数, \theta的先验密度函数为 \pi(\theta), 而抽样信息算得的后验密度函数与 \pi(\theta) 具有相同的函数形式,则称 \pi(\theta) 为 \theta 的共轭先验分布。

## 概率和似然的区别

- 概率是指在给定参数 \theta 的情况下, 样本的随机向量 X=x 的可能性。而似然表示的是在给定样本 X=x 的情况下,参数 \theta 为真实值的可能性。
- 一般情况,对随机变量的取值用概率表示。而在非贝叶斯统计的情况下,参数为一个实数而不是随机变量,一般用似然来表示。

## 频率学派和贝叶斯学派的区别

- 频率派认为抽样是无限的,在无限的抽样中,对于决策的规则可以很精确。贝叶斯派认为世界无时无刻不在改变,未知的变量和事件都有一定的概率,即**后验概率是先验概率的修正**。
- 频率派认为模型参数是固定的,一个模型在无数次抽样后,参数是不变的。贝叶斯学派认为数据才是固定的而参数并不是。
- 频率派认为模型不存在先验而贝叶斯派认为模型存在先验

## 0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器

- 0~1的均匀分布是均值为1/2，方差为0. 转成均值为0，方差为1.

## 矩阵正定性的判断,Hessian矩阵正定性在梯度下降中的应用

- 若矩阵所有特征值均不小于0,则判定为半正定。若矩阵所有特征值均大于0,则判定为正定。
- 在判断优化算法的可行性时Hessian矩阵的正定性起到了很大的作用,若Hessian正定,则函数的二阶偏导恒大于0,函数的变化率处于递增状态,在牛顿法等梯度下降的方法中,Hessian矩阵的正定性可以很容易的判断函数是否可收敛到局部或全局最优解。


## 讲一下PCA

- PCA (主成分分析) 是比较常见的线性降维方法,通过线性投影将高维数据映射到低维数据中,所期望的是在投影的维度上,新特征自身的方差尽量大,方差越大特征越有效,尽量使产生的新特征间的相关性越小。
- PCA算法的具体操作为对所有的样本进行中心化操作,计算样本的协方差矩阵,然后对协方差矩阵做特征值分解,取最大的n个特征值对应的特征向量构造投影矩阵。

## 22

## 脊回归模型(Ridge Regression)，通过调整正则化参数λ，来调整模型复杂度。当λ较大时，偏差（bias）和方差（variance）如何变化?

- λ越大，对模型中参数的惩罚力度越大，因此会有更多的参数被训练为0，模型也就变得更加简单了。
- 模型复杂度降低，偏差增大，方差减小.

## 正则化的作用

- 正则化可以防止过拟合
- L1正则化能得到稀疏解
- L2正则化约束了解空间

## 机器学习模型过拟合时，可行的操作包括:

- 降低特征维度
- 增加样本数量
- 增加正则项

## 随机森林与梯度提升树(GBDT)

- 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成。
- 随机森林不需要进行数据预处理，即特征归一化。而GBDT则需要进行特征归一化。
- 组成随机森林的树可以并行生成，而GBDT是串行生成。

## 判别式模型与生成式模型

- 常见判别式模型有：线性回归，决策树，SVM，k近邻，神经网络
- 常见生成式模型有：HMM，朴素贝叶斯，GMM，LDA

## 影响基本K-均值算法的主要因素有

- 样本输入顺序
- 模式相似度测度
- 初始聚类中心的选取
