# 数据科学, 机器学习面试

## 目录

[解释机器学习中的偏差和方差，并提出解决他们带来的问题的方法.](#解释机器学习中的偏差和方差，并提出解决他们带来的问题的方法)
[机器学习中的偏差-方差权衡是什么?](#机器学习中的偏差-方差权衡是什么)
[什么是梯度下降?](#什么是梯度下降)
[假设检验的一般步骤.](#假设检验的一般步骤)

## 正文

解释机器学习中的偏差和方差，并提出解决他们带来的问题的方法
===

`偏差`度量了学习算法的期望预测与真实结果的偏离程度, 即刻画了学习算法本身的拟合能力; `方差`度量了同样大小的训练集的变动所导致的学习性能的变化, 即刻画了数据扰动所造成的影响( ;`噪声`表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界, 即刻画了学习问题本身的难度). (西瓜书 2.5)

`Bias` measures the expected deviation from the true value of the function or parameter. `Variance` on the other hand, provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause. (deeplearningbook 5.4.4)

`Low Bias`: Suggests less assumptions about the form of the target function.
`High Bias`: Suggests more assumptions about the form of the target function.
`Low Variance`: Suggests small changes to the estimate of the target function with changes to the training dataset.
`High Variance`: Suggests large changes to the estimate of the target function with changes to the training dataset. [引用出处](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/)

机器学习中的偏差-方差权衡是什么
===

偏差-方差权衡是机器学习模型的一个属性, 它来源于偏差与方差的冲突: 减小偏差会增大方差, 减小方差会增大偏差, 无法同时减小二者. [归纳自维基百科](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)

泛化性能由学习算法的能力, 数据的充分性以及学习任务本身的难度共同决定. 给定学习任务, 为了取得好的泛化性能, 需要使偏差较小, 即能够充分拟合数据, 并使方差较小, 即使得数据扰动产生的影响小. 一般来说, 偏差和方差是冲突的. 训练不足时, 学习器的拟合能力不够, 训练数据的扰动不足以使学习器产生显著变化, 此时偏差主导了泛化错误率; 随着训练程度的加深, 学习器的拟合能力逐渐增强, 训练数据发生的扰动渐渐能被学习器学到, 方差逐渐主导了泛化错误率; 在训练程度充足后, 学习器的拟合能力已经非常强, 训练数据发生的轻微扰动都会导致学习器发生显著变化, 若训练数据本身, 非全局的特性被学习器学到了, 则将发生过拟合. (西瓜书 2.5)

![bias-variance trade-off](materials/imgs/bias-variance_trade-off.png)
<small>[deeplearningbook Figure5.6](https://www.deeplearningbook.org/contents/ml.html)</small>

The most common way to negotiate this trade-oﬀ is to use **cross-validation**. Alternatively, we can compare the **mean squared error(MSE)** of the estimates: The MSE measures the overall expected deviation between the estimator and the true value of the parameter $\theta$. Evaluating the MSE incorporates both the bias and the variance. Desirable estimators are those with small MSE error and these are estimators that manage to keep both their bias and variance somewhat in check. (deeplearningbook 5.4.4)

判断偏差-方差权衡最常用的方法是**交叉验证**. 另外, 也可以比较这些估计的**均方误差 MSE**, 它度量了估计和真实参数$\theta$之间的平方误差的总体期望偏差. MSE 估计包含了偏差和方差, 理想的估计具有较小的 MSE, 或者在检查中会稍微约束它们的偏差和方差. (深度学习 5.4.4)

什么是梯度下降
===

大多数深度学习算法都涉及某种形式的优化. **优化**指的是, 改变 x 以最大化或最小化某个函数 $f(x)$ 的任务. 将要最大化或最小化的函数称为**目标函数**, 或者**准则 criterion**. 当对其进行最小化时, 也把它称为**代价函数**, **损失函数**或**误差函数**. 利用损失函数的**导数**来指导参数更新, 实现最小化函数的技术就称为**梯度下降 gradient descent**. 导数代表 $f(x)$ 在点 x 处的斜率, 它表明如何缩放输入的小变化才能在输出获得相应的变化. 梯度下降利用了这一特点, 将 x 往导数的反方向移动一小步来减小 $f(x)$. 对于多维输入的函数 (输入向量), 梯度是相对于一个向量求导的导数, 是包含所有偏导数的向量. (汇总自深度学习 4.3)

假设检验的一般步骤
===

> **统计假设检验 (hypothesis test)** 为进行学习器性能比较提供了重要依据. 基于假设检验结果可以推断出, 若在测试集上观察到学习器 A 比 B 好, 则 A 的泛化性能是否在统计意义上优于 B, 以及这个结论的把握有多大. (西瓜书 2.4)

假设检验的一般步骤如下 [维基百科](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing):

1. 研究之初, 假设真相不明;
2. 提出相关的**零假设 null hypothesis**和**备择假设 alternative hypothesis**;
3. 考虑检验中对样本作出的**统计假设 statistical assumptions**, 比如独立性假设 (#???, 不太懂是什么意思);
4. 决定哪个检验是合适的, 并确定相关的**检验统计量 test statistic**;
5. 在零假设下推导检验统计量的分布. 在标准情况下, 应得到一个熟知的结果, 比如检验统计量可能符合正态分布;
6. 选择一个显著性水平, 若低于这个阈值, 就会拒绝零假设, 常用的是 5% 和 1%;
7. 根据在零假设成立时的检验统计量分布, 找到数值最接近备择假设, 且机率为显著性水平的区域 (**拒绝域 critical region**);
8. 针对检验统计量, 根据样本计算其估计值;
9. 若估计值未落在拒绝域, 接受零假设, 否则拒绝零假设.

维基百科上另一种更简短的过程:

1. (提出零假设;)
2. 基于观测, 计算出检验统计量的观测值;
3. 计算 **p值**;
4. 若 p值小于显著性水平, 拒绝零假设, 接受备择假设.

> 对于给定的统计模型, **p值 p-value (probability value)** 是零假设成立的的概率. [维基百科](https://en.wikipedia.org/wiki/P-value)

LSTM/GRU 如何缓解梯度消失或爆炸
===

- LSTM/GRU, 通过门控, 动态地调整每个时间步的连接权.
- LSTM 引入了自循环 (从cell 到 cell?), 以产生梯度长时间持续流动的路径, 缓解了梯度消失和爆炸问题. 通过门控, 自循环的权可以动态地调整.

如何理解 ROC 和 AUC
===

- ROC, Receiver operating characteristic, 受试者工作特征, 纵轴是真正率, 横轴是假正率.
- 进行模型比较时, 若一个模型的 ROC 被另一个的完全包住, 可以断言后者的性能优于前者; 若 ROC 曲线存在交叉, 一般难以判断两者的优劣. AUC 是此情况下较为合理的判据.

查全率和查准率如何计算
===

- recall = TP/(TP+FN)   # 假反例, 也就是正例被误判为反例, 于是分母就构成了正例的总数, 这也正是翻译查全率的由来, 要尽可能多地将正例揪出来
- precision = TP/(TP+FP)  # 被预测为正例的样本中, 真正例的比例. (可以记成, 混淆矩阵的第一列)
- F1 = 2/(recall^{-1}+precision^{-1})=2\*(precision\*recall)/(precision+recall)
- precision-recall tradeoff, 一般来说, 当 F 值固定之后, 提升 precision 会带来 recall 的降低, 而提高 recall 会带来 precision 的降低, 两者之间存在一个动态的平衡.

Bagging 和 Boosting 的区别
===

1. 样本选择上：
  - Bagging ：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
  - Boosting ：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
2. 样例权重：
  - Bagging ：使用均匀取样，每个样例的权重相等
  - Boosting ：根据错误率不断调整样例的权值，错误率越大则权重越大。
3. 预测函数：
  - Bagging ：所有预测函数的权重相等。
  - Boosting ：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
4. 并行计算：
  - Bagging ：各个预测函数可以并行生成
  - Boosting ：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

LDA
===

- LDA 是一种主题模型，它可以将文档集中每篇文档的主题按照概率分布的形式给出，对于每一个主题均可找出一些词语来描述它。
- 每个主题都表示为词汇的dirichlet(狄利克雷)分布
- 每个文档都表示为主题的dirichlet分布
- 求解LDA模型常用Gibbs(吉布斯)采样

K-means vs KNN
===

- KNN 是一种用于分类和回归的无参数模型, 属于监督学习方法!!!
  - 在k-NN分类中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，k个最近邻居（k为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。若k = 1，则该对象的类别直接由最近的一个节点赋予。
  - 在k-NN回归中，输出是该对象的属性值。该值是其k个最近邻居的值的平均值。
- k-means 是一个聚类算法, 属于无监督学习方法. 它把样本划分到k个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准。

独立变量之间的期望方差计算
===

- E(XY)=E(X)E(Y), 独立变量的乘积的期望=期望的乘积
- E(X+Y)=E(X)+E(Y), 独立变量的和的期望=期望的和
- D(X+Y)=D(X)+D(Y), 独立变量的和的方差=方差的和

常见的决策树算法, 及其特点
===

- 特征选择的标准，有信息增益和基尼系数两种
- `CART` 以基尼系数选择切分特征, 运行速度一般, 后剪枝, 支持缺失值处理
- `ID3` 以信息增益选择切分特征, 运行速度慢, 不支持剪枝, 不支持缺失值处理
- `C4.5` 以信息增益率选择切分特征, 运行速度快于 ID3, 预剪枝, 不支持缺失值处理
- `C5.0` 以信息增益率选择切分特征, 运行速度最快, 预剪枝, 支持缺失值处理
- (信息增益对可取数目较多的属性有所偏好。 增益率对可取数目较少的属性有所偏好。 )

激活函数的作用
===

- 神经元的工作, 简单地说, 就是计算了输入的加权和, 再加上一个偏置. 不带激活函数的情况下, 一个神经网络层就是对输入进行了线性变换, 网络的表达能力不够, 不能解决非线性问题, 比如经典的 XOR 问题. 因此引入了非线性激活函数, 提高神经网络模型的表达能力, 解决线性模型不能解决的问题.
- 对于 CNN 而言, 激活函数加在卷积层之后, PyTorch 实现的 ResNet, 实际上在卷积和激活函数之间还有一层 BN 层, 然后才是激活函数之后的池化层.

Max pooling 的梯度如何反向传播
===

- 非最大值单元的梯度都为 0 , 梯度只沿着最大值反向传播. 一个直观的理解是, 只要不是最大值, 它们再怎么闹, 对结果都没有影响, 也就不必调整它们对应的路径上的参数了.

Logistic Regression 的损失函数
===

- ![logistic regression cost function](materials/imgs/logistic_regression_cost_function.png)

Dropout 的形式以及作用
===

- Dropout, 在训练阶段, 每个神经元以概率 p 失活, 连通着它的入边和出边也都被丢弃了. (不同于 DropConnection, 只是丢弃了神经元的部分入边)
- 训练阶段, 神经元以概率 p 被丢弃; 预测阶段, 不丢弃神经元, 但是它们的输出会相应地减少 p. 还有一种做法是反过来, 训练时, 以概率 p 丢弃神经元后, 未丢弃的神经元的输出放大 1-p 倍, 此时, 预测阶段, 神经元的输出不变.
- 通过每次迭代随机地丢弃神经元, Dropout 能减轻神经元之间的相互依赖, 从而有效地防止过拟合.
- Dropout 能鼓励神经网络学习到更加鲁棒的特征, 这些特征对 dropout 后的子神经网络都有用.
- Dropout 的做法类似 bagging, 因此 dropout 也可以被认为是一种实用的大规模深度神经网络的模型集成方法. Dropout 在小批量级别上操作, 提供了一种轻量级 bagging 集成近似, 能够实现指数级数量神经网络的训练与评测. 对一个具有 H 个神经元的神经网络, 每个神经元都可以保留或者丢弃, 所以理论上可以有 2^H 个子网络.

SVM 和 LR 的异同
===

- 共同点:
    - LR 和 SVM 都能用于分类
    - 如果不考虑核函数, LR 和 SVM 都是线性分类器, 即它们的分类决策面都是线性的
    - LR 和 SVM 都是监督学习方法
    - LR 和 SVM 都是判别模型, 计算条件概率分布 P(Y|X), 即它们不关心数据是如何产生的, 只关心数据之间的差别, 用差别来进行分类
        - 常见的判别式模型: KNN, SVM, LR
        - 常见的生成式模型: 朴素贝叶斯, 隐马尔可夫模型

    - LR 和 SVM 都不复杂, 在学术界和工业界都广为认知, 并得到了广泛应用
- 不同点
    - 本质的不同是: 损失函数不同. 逻辑回归基于`概率论`, 假设样本为 1 的概率可以用 sigmoid 函数表示, 通过极大似然估计的方式估计出参数值; 支持向量机基于`几何间隔最大化原理`, 认为存在最大几何间隔的分类面为最优分类面.
    - SVM 只考虑局部的边界线附近的点, 而 LR 考虑全局数据. 换言之, 影响 SVM 决策面的样本只有少数的支持向量, 而 lR 中, 每个样本都对决策面有影响. 所以 SVM 不直接依赖于数据分布, LR 则受所有数据的影响.
    - 在解决非线性问题时, SVM 采用核函数机制, LR 则不采用核函数的方法. 由于 SVM 只有少数支持向量影响决策面, 因此只有少数样本参与核计算; 而 LR 中每个样本都参与决策面的计算, 如果也使用核函数, 每个样本都参与核计算, 计算复杂度高.
    - 线性 SVM 依赖数据表达的距离测度, 需要先对数据做 normalization, 而 LR 不受影响.
    - SVM 的损失函数自带正则, 所以 SVM 是结构风险最小的算法的原因, LR 需要另外添加正则项.

Normalization 和 Standardization 的区别
===

- Normalization 的处理通常是: 减去最小值再除以区别长度, 将数据缩放到 [0, 1] 的区间内
- Standardization 的处理是, 减去均值再除以标准差, 将数据缩放成服从以 0 为均值, 1 为标准差的正太分布
- 归一化不改变数据的分布; 标准化会改变数据的分布
- 归一化是一种简化计算, 通常用于将有量纲的表达式转化成无量纲的表达式, 变成纯量, `消除量纲对模型的影响`, 使不同变量具有可比性
- 标准化能将狭长的数据分布转变成类圆型的, 使不同特征对目标函数的影响权重一致, 能够加快模型收敛, 同时提高解的精度.

RF, GBDT, XGBoost 的异同
===

- RF、GBDT和XGBoost都属于集成学习，目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。
- 根据个体学习器的生成方式，目前的集成学习方法大致分为两大类：即个体学习器之间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表就是Boosting，后者的代表是Bagging和“随机森林”。(西瓜书, P173)
- Bagging 使用有放回地抽样, 得到 T 个 m 个样本(可能存在重复)的采样集, 基于每个采样集训练一个基学习器, 再将基学习器进行组合. 对于分类, 使用简单投票法; 对于回归, 使用简单平均法
- RF 是 bagging 的扩展变态, 在以决策树为基学习器构建 bagging 集成的基础上, 进一步在决策树的训练过程中引入了随机属性选择. 概括地讲, RF 包括四部份: 1) 随机采样 (有放回); 2) 随机特征选择 (对基决策树的每个节点, 先从节点的属性集合中随机选择一个包含 k 个属性的子集, 再从子集中选择一个最优属性用于划分); 3) 构建决策树; 4) 随机森林投票
- RF和Bagging对比：RF的起始性能较差，特别当只有一个基学习器时，随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的‘随机型’决策树则只考察一个属性子集. (西瓜书, P181)
- Boosting 中, 不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练, 通过关注被已有分类器错分的那些数据来获得新的分类器。
- Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。
- GradientBoost与传统的Boosting区别较大，前者的每一次计算都是为了减少上一次的残差，在残差减小的梯度方向上建立模型,后者则关注正确错误的样本加权。 
- GBDT(Gradient Boosted Decision Trees)会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）.
- GBDT 与 XGBoos 的区别
    1. 传统GBDT以CART作为`基分类器`，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
    2. 传统GBDT在优化时只用到一阶`导数信息`，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
    3. xgboost在代价函数里加入了`正则项`，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
    4. `Shrinkage（缩减）`，相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
    5. `列抽样（column subsampling）`。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
    6. `对缺失值的处理`。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
    7. xgboost工具支持`并行`。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
    8. `可并行的近似直方图算法`。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

K-means 如何确定 K 值
===

- 从数据本身的特征来讲，最佳K值对应的类别下应该是**类内距离最小化**并且**类间距离最大化**。有多个指标可以用来评估这种特征，比如`平均轮廓系数、`类内距离/类间距离`等都可以做此类评估。
- **手肘法**: 随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么`误差平方和SSE`自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类. (枚举 K, 计算对应的误差平方和 SSE)
- **轮廓系数法**: 轮廓系数表示为 $S=(b-a)/max(b,a)$, a 是样本与同簇其他样本的平均距离, 称为凝聚度; b 为样本与最近簇中所有样本的平均距离, 称为分离度. 求出所有样本的轮廓系数再求平均就得到了`平均轮廓系数`, 凝聚度越高, 分离度越高, 平均轮廓系数越大, 聚类效果越好. (还是枚举 K, 在计算轮廓系数)

RNN vs. Hidden Markov Model
===

- RNN 和 HMM 都可以用来处理时间序列问题
- HMM 是一个关于时序的概率模型, 描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列, 再由各个状态生成一个观测而产生观测随机序列的过程. HMM 由`初始状态概率向量`, `状态转移概率矩阵`和`观测概率矩阵`3者共同决定.
- HMM 有两个基本假设:
    1. 齐次马尔可夫性假设: 假设隐藏的马尔可夫链在任意时刻的状态都只依赖前一时刻的状态, 与其他时刻的状态及观测无关, 也与时刻无关.
    2. 观测独立性假设: 即假设任意时刻的观测只依赖于该时刻的马尔可夫状态, 与其他观测及状态无关.
- HMM 与 RNN 的异同
    - HMM 比起 RNN 相对更简单
    - HMM 的马尔可夫性假设是一个很强的假设, 即当前状态仅依赖于前一时刻的状态; 而 RNN 释放了这一假设, 可以考虑更长的依赖.
    - HMM 是生成式模型, 而 RNN 主要是判别式模型
    - HMM 的隐藏状态是 one-hot 形式的, RNN 的分布式表示的, 这意味着它能存储更多的信息, 表达能力更强

Batch Normalization 是怎样的, 有什么作用
===

- BN 对前一层的输出 (根据放置的位置, 前一层可能是线性层, ResNet 的 BN 夹在线性层和非线性激活层之间, 也可能是激活层), 减去当前批量的均值再除以标准差. 做法和对输入特征的 standardization 一样, 所以 BN 可以看作每一层前的预处理.
- 同 Standardization, BN 能加速模型收敛, 同时提高解的精度. 另外比较公认的一点是, BN 带轻微的正则化效果, 有助于防止过拟合, 这一点可以这样理解: 它相当于对数据分布做了约束, 就像 L1L2 正则化的约束一样
- normalization 会改变表示, 破坏了之前学习到的特征分布, 有时候我们并不需要均值为0, 标准差为 1 的正态分布, 因此 BN 层加了两个参数, 一个表示标准差, 一个表示均值, $y = \gamma*x + \beta$. 在 gamma 和 beta 的帮助下, 可以恢复最优的输入数据分布. 因为 gamma 和 beta 随着整个数据集学习, 相当于与当前批数据的特征进行了解耦, 实际上有利于优化过程, 提高模型的泛化能力. 一来一回, 有点降噪的功效.
- BN 将所有批数据强制在了统一的数据分布下
- 推断时, BN 不再计算当前 batch 的 mean 和 std (可以假设每次只有单条数据), 而是用了整个训练集上估计得到的 mean 和 std, 按照文章的说法, 可以用`移动平均`估计得到.
- BN 的优点:
    1. 改善梯度的反向传播;
    2. 允许使用更大的学习率;
    3. 减少对参数初始化的依赖;
    4. 带正则化效果, 可以减小 dropout 的丢弃概率

如何处理缺失值
===

0. 不处理, 部分模型能处理缺失值
1. 固定值填充
2. 均值填充
3. 众数填充
4. 前后数据填充 (需要前后数据之间有点关系)
5. 插值填充  (用 Pandas 的话, 可以一键插值: `df.interpolate()`)
6. 用算法拟合  (用其他变量做预测模型来算出缺失变量, 有一个根本缺陷，如果其他变量和缺失变量无关，则预测的结果无意义。如果预测结果相当准确，则又说明这个变量是没必要加入建模的。一般情况下，介于两者之间。)
    1. 用 kNN 进行填充 (fancyimpute 一个神奇的包)
    2. 用随机森林进行填充
7. 将变量映射到高维空间 (向量化, 嵌入) (fastai 有介绍这样的方法, 大厂的 CTR 都这样处理, 好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。)

NLP 中的概率图模型
===

- 概率图模型分为贝叶斯网络和马尔科夫网络, 贝叶斯网络可以用一个有向图结构表示, 马尔科夫网络可以用无向图结构
- 进一步, 概率图模型包含朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等
- 朴素贝叶斯模型:
    - 目标函数: 后验概率最大化, 等价于 01 损失最小
    - NB 的参数学习意味着需要估计 P(x|y_k) 和 P(y_k), 采用极大似然估计 (为解决可能出现的概率为 0 的情况, 可采用 add-1-smoothing 或 add-k-smoothing)
- 高斯混合模型
    - 聚类算法, 使用 EM 算法进行学习
    - 假设每个簇的分布都服从高斯分布
- HMM
    - 1 个参数: lambda = {初始状态概率向量 pi, 状态转移概率矩阵 A, 观测概率矩阵 B}
    - 2 个假设: 齐次马尔可夫假设+观测独立性假设
    - 3 个问题: 概率计算问题(直接计算法, 前向算法, 后向算法), 学习问题(监督学习法, Baum-Welch算法), 预测问题(近似算法, 维特比算法)
    - 生成模型
- 最大熵模型
    - 最大熵原理: 在所有可能的概率模型的集合中, 熵最大的模型是最好的
- CRF 
    - 去除了 HMM 的两个假设, 模型更加复杂
    - 计算全局最优输出节点的概率
    - 判别模型
- CRF vs HMM vs MEMM (最大熵马尔可夫模型)
    - 都常用来做序列标注的建模，像分词、词性标注，以及命名实体标注
    - HMM 一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择
    - MEMM 解决了 HMM 的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉。
    - CRF 并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。

ReLU vs Sigmoid vs Tahn
===

- ReLU 的优点
    - 从计算的角度上，Sigmoid 和 Tanh激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值, **计算简单**
    - ReLU 的**非饱和性**可以有效地解决梯度消失的问题，提供相对宽的激活边界
    - ReLU的单侧抑制提供了网络的**稀疏表达能力**
- ReLU 的缺点
    - 训练过程中会**导致神经元死亡**, 因为负梯度传递到 ReLU 时被置 0, 且之后都不会被任何数据激活. 实际训练中, 如果学习率太大, 会导致超过一定比例的神经元不可逆死亡, 导致训练过程失败. (小比例的神经元死亡体现了稀疏表达能力?)       - ReLU 的一些变种正是为解决这个问题发明的, 比如 LeakyReLU, 即保留了单侧抑制, 又保留了部分负梯度信息以致不完全丢失

神经网络的结构及特点
===

- 前馈网络：整个网络中的信息只朝一个方向传播，没有反向的信息传播，可以用一个**有向无环路图**表示。前馈网络包括全连接前馈网络和卷积神经网络等
- 反馈网络：反馈网络中神经元不但可以接收其它神经元的信号，也可以接收自己的反馈信号。和前馈网络相比，反馈网络中的神经元具有记忆功能，在不同的时刻具有不同的状态。反馈神经网络中的信息传播可以是单向或双向传递，因此可用一个有向循环图或无向图来表示。反馈网络包括循环神经网络、Hopfield 网络、玻尔兹曼机等
 图网络：图网络是定义在**图结构**数据上的神经网络。图中每个节点都一个或一组神经元构成。节点之间的连接可以是有向的，也可以是无向的。每个 节点可以收到来自相邻节点或自身的信息。 图网络是前馈网络和记忆网络的泛化，包含很多不同的实现方式，比如图卷积网络、消息传递网络等

如何理解通用近似定理
===

- 描述: 对于具有**线性输出层**和至少一个使用**“挤压”性质的激活函数**的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何从一个定义在实数空间中的有界闭集函数。
- 所谓“挤压”性质的函数是指像 sigmoid 函数的有界函数，但神经网络的通用近似性质也被证明对于其它类型的激活函数，比如ReLU，也都是适用的。
- 通用近似定理只是说明了神经网络的计算能力可以去近似一个给定的连续函数，但并**没有给出如何找到这样一个网络，以及是否是最优的**。此外，当应用到机器学习时，真实的映射函数并不知道，一般是通过经验风险最小化和正则化来进行参数学习。因为神经网络的强大能力，反而容易在训练集上过拟合。

如何理解误差的反向传播
===

- 具体含义: 第 l 层的一个神经元的误差项是所有与该神经元相连的第 l+1 层的神经元的误差项的权重和，然后再乘上该神经元激活函数的梯度

卷积神经网络的组件有哪些, 起什么作用
===

- 卷积层: 局部连接, 权值共享
- 池化层: 特征选择, 降低特征数量从而减少参数量
- 全连接层

在深度学习中，网络层数增多会伴随哪些问题，怎么解决？为什么要采取残差网络ResNet？
===

- 网络层数加深可能伴随的问题:
    1. 计算资源的消耗
    2. 模型容易过拟合
    3. 梯度消失/爆炸
    4. 退化现象: 不同于过拟合(训练集上的 loss 一直在下降), 退化现象是: 训练集上 loss 下降之后, 逐渐趋于饱和.
- 残差网络
    - 将目标函数拆分成两部分: 恒等函数+残差函数 `h(x) = x+(h(x)-x)`
    - 依据: 通用近似原理, 一个由神经网络构成的非线性单元有足够的能力来近似逼近原始目标函数或残差函数, 而后者更容易学习.
    - 属性: L 层可以表示成任意一个比它浅的 l 层和它们之间的残差部分之和. 于是, L 层的梯度可以直接传递到任何一个比它浅的 l 层, 相当于缩短了误差反向传播的链, 能有效缓解梯度消失/爆炸问题.

RNN 如何进行参数学习
===

- 通过`随时间反向传播 BPTT`算法进行误差的反向传播. 按时间展开之后, RNN 可以看作一个多层前馈网络, 一层对应一个时刻.

RNN 长期依赖问题的原因
===

- 时间序列可能很长, 根据 BPTT 算法, 误差随时间反向传播的链可能很长. 根据链式法则, 此时, 即使梯度是略大于 1 的值, 经过累乘之后, 变得极大, 导致梯度爆炸; 而梯度小于 1 时, 经过累乘, 梯度会趋向于 0, 导致梯度消失
- 对前馈神经网络, 梯度消失意味着, 无法通过加深网络层次来改善神经网络的预测效果, 因为此时只有靠近输出层的几层能得到较好的学习
- 梯度爆炸问题可以通过`梯度裁剪`来缓解, 即梯度的范数大于某个给定值时, 就对梯度进行等比缩放
- 梯度消失问题, 只能通过调整模型本身进行改进.
    - 深度残差网络是对前馈网络的改进, 通过残差学习的方式缓解了梯度消失问题;
    - 对于 RNN, GRU 和 LSTM 等通过加入门控机制来缓解梯度消失问题

普通前馈网络或 CNN 通过采取 ReLU 通常可以有效改善梯度消失问题(非饱和特性), RNN 为什么采用 tanh 而不是 ReLU
===

- RNN 可以使用 ReLU 作为激活函数, 但是要对隐藏状态的连接矩阵的初始值做一定的限制, 否则容易引发数值问题.
    - 假设 RNN 采用了 ReLU, 且一直处于激活区域, 即 f(x)=x, 则`net_{t}=Ux_{t}+Wf(Ux_{t-1}+Wh_{t-2})`, 将其展开, net 的表达式将包含多个 W 的连乘, 若 W 不是单位矩阵, 最终的结果会趋于 0 或者无穷, 引发数值问题.
    - 误差反向传播的情况类似
    - 实验证明, 将上述 W 初始化为单位矩阵并使用 ReLU, 在一些应用中取得了与 LSTM 相似的结果, 且学习速度更快
- 前馈网络不会出现这种问题, 是因为每一层的权重矩阵是不同的, 初始化时, 它们是独立同分布的, 可以相互抵消, 多层之后一般不一会出现严重的数值问题

LSTM 如何实现长短期记忆功能
===

- LSTM 的 cell state 可以看作保存了长期记忆, 而 hidden state 保存了短期记忆
    - 在(一个训练好的) LSTM 中, 如果输入的序列没有重要的信息, 遗忘门的值接近于 1, 输入门的值接近于 0, 此时过去的记忆会被保存, 实现了长期记忆功能;
    - 当输入的序列中出现重要信息时, LSTM 会把它存入记忆中, 此时输入门的值会接近于 1;
    - 当输入的序列中出现重要信息, 且该信息意味着之前的信息不再重要, 输入门的值接近于 1, 而遗忘门的值接近于 0, 这样旧的记忆就被遗忘了, 新的重要信息被记忆

深度神经网络优化的难点有哪些
===

- 深度神经网络是高度非线性的模型, 其风险函数是一个非凸函数, 因此风险最小化是一个非凸优化问题, 会存在很多局部最优点.
- 在高维空间中, 非凸优化的难点并不在于如何逃离局部最优点, 而是如何逃离鞍点

神经网络数据预处理方法有哪些
===

- Normalization
- Standardization
- 白化: 降低输入特征之间的冗余性, 经白化后, 特征之间的相关性较低, 并且所有特征具有相同的方差.

参数初始化为 0 , 过大或过小会怎么样
===

- 考虑全连接的深度神经网络, 同一层中任意神经元都是**同构的**, 它们拥有相同的输入和输出, 如果再将参数全部初始化为同样的值, 那么无论前向传播还是反向传播的取值都是完全相同的, 即存在对称现象, 且永远无法打破, 无论如何训练, 同一层网络的各个参数仍然是相同的.
    - 偏置可以被简单地设为 0, 不会导致参数对称问题
- 对于饱和性激活函数而言, 参数设置过大, 激活值容易饱和, 从而导致梯度消失; 参数设置过小, 激活函数就失去了分线性能力

常用的初始化方法有哪些
===

- 高斯分布: N(mean, std2)
- 均匀分布: U(a, b)
- Xavier 高斯分布: N(0, std2), std=gain x sqrt(2/(fan_in, fan_out))
- Xavier 均匀分布: U(-a, a), a=gain x sqrt(6/(fan_in, fan_out))
- Kaiming 高斯分布: N(0, std2), std=sqrt(2/((1+a^2)xfan_in))
- Kaiming 均匀分布: U(-bound, bound), bound=sqrt(6/((1+a^2)xfan_in))

神经网络优化方法有哪些
===

- 学习率衰减
    - AdaGrad: 累计过去所有的梯度平方. 缺点是在经过一定次数的迭代依然没有找到最优点时, 由于学习率很小了, 很难再继续找到最优点.
    - RMSprop: 每次都计算梯度平方的指数衰减移动平均
    - AdaDelta
- 梯度方向优化
    - 动量法: 用梯度的移动平均来代替每次的实际梯度
    - Nesterov 加速梯度
    - 梯度截断
- Adam: 一方面和 RMSprop 类似, 计算梯度平方的移动平均; 另一方面计算梯度的移动平均
